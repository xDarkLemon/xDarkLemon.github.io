<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Yibo Liu</title>
    <meta name="description" content="Personal webpage">
    <link rel="icon" href="/logo.png">
    
    <link rel="preload" href="/assets/css/0.styles.eeb2254b.css" as="style"><link rel="preload" href="/assets/js/app.048c192b.js" as="script"><link rel="preload" href="/assets/js/2.4d7c2de1.js" as="script"><link rel="preload" href="/assets/js/7.ed84a659.js" as="script"><link rel="preload" href="/assets/js/4.9e6fd5e0.js" as="script"><link rel="preload" href="/assets/js/5.af4ec4c0.js" as="script"><link rel="prefetch" href="/assets/js/10.8c8f8ff3.js"><link rel="prefetch" href="/assets/js/11.73363121.js"><link rel="prefetch" href="/assets/js/12.6a109984.js"><link rel="prefetch" href="/assets/js/13.3e5575c6.js"><link rel="prefetch" href="/assets/js/14.bcda478a.js"><link rel="prefetch" href="/assets/js/15.9276889a.js"><link rel="prefetch" href="/assets/js/16.d2bdd84d.js"><link rel="prefetch" href="/assets/js/17.d239630a.js"><link rel="prefetch" href="/assets/js/18.8645afb5.js"><link rel="prefetch" href="/assets/js/3.a28516b4.js"><link rel="prefetch" href="/assets/js/6.d86c8b7f.js"><link rel="prefetch" href="/assets/js/8.250fbf9b.js"><link rel="prefetch" href="/assets/js/9.2147743f.js">
    <link rel="stylesheet" href="/assets/css/0.styles.eeb2254b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Yibo Liu</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link router-link-exact-active router-link-active">Home</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link router-link-exact-active router-link-active">Home</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/myprofile2.jpeg" alt></div> <div class="info"><div class="name">
      Yibo Liu
    </div> <div class="bio"><p>PhD student in Computer Science</p></div> <div class="socials"><div><a href="https://scholar.google.com/citations?user=FQExy98AAAAJ&amp;hl=en&amp;oi=ao" target="_blank"><img src="/icons/google-scholar.svg" alt="google-scholar" title="google-scholar"></a></div><div><a href="https://github.com/xDarkLemon" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div><div><a href="https://www.linkedin.com/in/yibo-liu-5a60a21b5/" target="_blank"><img src="/icons/linkedin.svg" alt="linkedin" title="linkedin"></a></div><div><a href="https://twitter.com/_liuyibo" target="_blank"><img src="/icons/twitter.svg" alt="twitter" title="twitter"></a></div></div> <div class="contact"><div title="Contact me" class="email">yl6769 (at) nyu (dot) edu</div></div> <div><a target="_blank" href="/files/Yibo_Liu_CV_GM.pdf" title="Download my CV in PDF"><font size="2em" color=""><b>[CV]</b></font></a></div></div></div> <h2 id="about">About</h2> <p>I am Yibo Liu, a second year Ph.D. student in Computer Science at University of Victoria, BC, Canada, supervised by <a href="http://web.uvic.ca/~teseo/" target="_blank" rel="noopener noreferrer">Prof. Dr. Teseo Schneider<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> . Prior to this, I obtained my M.S. in Computer Science from New York University, NY, USA in 2022,  and B.Eng. in Electronic Engineering from Beijing University of Posts and Telecommunications, Beijing, China, in 2019. My industrial experience includes an internship at <em>Data, Knowledge and Intelligence</em> group at Microsoft Research Asia. My current research interests include computer graphics and physics-inspired machine learning.</p> <h2 id="industrial-experience">Industrial Experience</h2> <ul><li><strong>Microsoft Research Asia</strong> | Beijing, China | Aug 2020 - Feb 2021  <br>
Intern at <em>Data, Knowledge and Intelligence</em> group, full-time, onsite
<ul><li>Conducted the research on Table2Charts, a charts recommendation methods based on excel tables.</li> <li>Delivered Table2Charts technique to Bing Search and to Excel Spreadsheet Intelligence.</li> <li>Designed multilingual key-phrase extraction algorithm for questionnaire word cloud used in Forms Ideas and in Teams poll.</li></ul></li></ul> <h2 id="research-experience">Research Experience</h2> <ul><li><p><strong>Geometric Computing Lab, New York University</strong> | NY, USA | Sept 2022 - Apr 2023 <br>
Independent project, supervised by <a href="http://web.uvic.ca/~teseo/" target="_blank" rel="noopener noreferrer">Prof. Dr. Teseo Schneider<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and <a href="https://cims.nyu.edu/gcl/daniele.html" target="_blank" rel="noopener noreferrer">Prof. Dr. Daniele Panozzo<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <br>
Worked on GPU accelerated contact simulations in PolyFEM library.</p></li> <li><p><strong>CILVR Lab, New York University</strong> | NY, USA | Mar 2020 - May 2021 <br>
Independent project, supervised by <a href="http://iacercalixto.github.io" target="_blank" rel="noopener noreferrer">Prof. Dr. Iacer Calixto<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and <a href="http://claravania.github.io" target="_blank" rel="noopener noreferrer">Dr. Clara Vania<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><br>
Worked on learning robust mulilingual multimodal knowledge graph representations.</p></li> <li><p><strong>Center for Speech and Language Technologies, Tsinghua University</strong> | Beijing, China | 2018 - 2019<br>
Research intern, supervised by <a href="http://wangd.cslt.org" target="_blank" rel="noopener noreferrer">Prof. Dr. Dong Wang<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <br>
Worked on ancient Chinese poetry generation.</p></li></ul> <h2 id="publication">Publication</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/01.png" alt></div> <div class="card-content"><p><strong>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</strong></p> <p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, <strong>Yibo Liu</strong>, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</p> <p><em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 9556-9567, Jun. 2024</p> <p>[<a href="https://arxiv.org/abs/2311.16502" target="_blank" rel="noopener noreferrer">Paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://mmmu-benchmark.github.io" target="_blank" rel="noopener noreferrer">Web Page<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/02.png" alt></div> <div class="card-content"><p><strong>Endowing Language Models with Multimodal Knowledge Graph Representations</strong></p> <p>Ningyuan Huang, Yash R. Deshpande, <strong>Yibo Liu</strong>, Houda Alberts, Kyunghyun Cho, Clara Vania, Iacer Calixto</p> <p><em>arXiv</em> 2206.13163, Jun. 2022</p> <p>[<a href="https://arxiv.org/abs/2206.13163" target="_blank" rel="noopener noreferrer">Paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/03.png" alt></div> <div class="card-content"><p><strong>VisualSem: a high-quality knowledge graph for vision and language</strong></p> <p>Houda Alberts, Ningyuan Huang, Yash Deshpande, <strong>Yibo Liu</strong>, Kyunghyun Cho, Clara Vania, Iacer Calixto</p> <p><em>Proceedings of the 1st Workshop on Multilingual Representation Learning</em>, pp. 138-152, Nov. 2021. <em>(colocated with EMNLP 2021)</em></p> <p>[<a href="https://aclanthology.org/2021.mrl-1.13.pdf" target="_blank" rel="noopener noreferrer">Paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/04.png" alt></div> <div class="card-content"><p><strong>Table2Charts: Recommending Charts by Learning Shared Table Representations</strong></p> <p>Mengyu Zhou, Qingtao Li, Xinyi He, Yuejiang Li , <strong>Yibo Liu</strong>, Wei Ji, Shi Han, Yining Chen, Daxin Jiang, Dongmei Zhang.</p> <p><em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, pp. 2389-2399, Aug. 2021</p> <p>[<a href="https://dl.acm.org/doi/pdf/10.1145/3447548.3467279" target="_blank" rel="noopener noreferrer">Paper<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/microsoft/Table2Charts" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="talks">Talks</h2> <p>I presented our paper <a href="/files/MRL_slides.pdf">&quot;VisualSem: a high-quality knowledge graph for vision and language&quot;</a> at <a href="https://www.aclweb.org/portal/content/1st-workshop-multilingual-representation-learning-mrl-emnlp" target="_blank" rel="noopener noreferrer">Workshop on Multilingual Representation Learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> 2021.</p> <h2 id="peer-review-service">Peer Review Service</h2> <p><strong>International Conference on Learning Representations (ICLR)</strong>, 2025</p> <p><strong>Transactions on Visualization and Computer Graphics (TVCG)</strong>, 2025</p> <p><strong>Transactions on Visualization and Computer Graphics (TVCG)</strong>, 2024</p> <h2 id="volunteer-experience">Volunteer Experience</h2> <p><strong>SIGGRAPH Asia 2024</strong> Student Volunteer | Tokyo, Japan | Dec 2024</p> <h2 id="teaching-assistantship">Teaching Assistantship</h2> <p><strong>CSC 503 &amp; SENG 474 Data Mining</strong>, University of Victoria, BC, Canada, 2025 spring</p> <p><strong>CSC 503 &amp; SENG 474 Data Mining</strong>, University of Victoria, BC, Canada, 2024 winter</p> <p><strong>CSC 116 Introduction to C++</strong>, University of Victoria, BC, Canada, 2024 fall</p> <p><strong>SENG 350 Software Architecture</strong>, University of Victoria, BC, Canada, 2024 fall</p> <p><strong>CSC 503 &amp; SENG 474 Data Mining</strong>, University of Victoria, BC, Canada, 2023 fall</p> <h2 id="open-source-projects">Open-Source Projects</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/06.png" alt></div> <div class="card-content"><p><strong>Lock-free Linked List Library for GPUs</strong></p> <ul><li>The <strong>first</strong> library supporting all singly linked list operations on GPUs with CUDA.</li> <li>Achieveing 141x speedup for insertions and deletions compared to sequential operations.</li> <li>Individual contribution.</li></ul> <p>[<a href="https://github.com/xDarkLemon/Lock_Free_Linked_List_GPU/tree/master" target="_blank" rel="noopener noreferrer">GitHub<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><div class="card-image"><img src="/projects/10.png" alt></div> <div class="card-content"><p><strong>COIG-PC: Chinese Open Instruction Generalist Prompt Collection</strong></p> <ul><li>Collected prompts to facilitate the fine-tuning and optimization of Chinese language models.</li></ul> <p>[<a href="https://huggingface.co/datasets/BAAI/COIG-PC" target="_blank" rel="noopener noreferrer">Huggingface Dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="blog-posts">Blog Posts</h2> <div class="md-card show-border"><div class="card-image"><img src="/projects/09.png" alt></div> <div class="card-content"><p><strong>Expressive Power, Generalization, and Optimization of Graph Neural Networks: A Survey</strong></p> <p>Summarized the theoretic frameworks of GNN’s expressive power; summarized the generalization bounds,
the generalization ability of different GNNs, and the methods to improve generalization ability; stated the
explanation of over-fitting problem and summarized the optimization methods.</p> <p>[<a href="/files/GNN_Survey.pdf">full article</a>]</p></div></div> <h2 id="hobbies">Hobbies</h2> <p><strong>Cycling</strong>: Completed a ride around Qinghai Lake covering a distance of 360 kilometers in 5 days in 2015.</p> <p><strong>Snowboarding</strong>: Intermediate level.</p> <p><strong>Bouldering</strong>: Beginner level.</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">12/19/2023, 8:40:08 PM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.048c192b.js" defer></script><script src="/assets/js/2.4d7c2de1.js" defer></script><script src="/assets/js/7.ed84a659.js" defer></script><script src="/assets/js/4.9e6fd5e0.js" defer></script><script src="/assets/js/5.af4ec4c0.js" defer></script>
  </body>
</html>
