<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="比赛介绍赛事链接：科大讯飞：鸟类鸣叫声识别挑战赛 任务描述：鸟类鸣叫声识别挑战赛旨在增强自动鸟类鸣叫声识别技术，预测出每个测试音频中出现的鸟类物种。测试音频文件只包含单一的鸟类物种，预测在音频文件级别进行，不需要开始和结束的时间戳，属于单标签分类任务。 数据说明：训练数据集包含100类鸟声数据，存在类别不均衡，真实背景噪音。不可以使用外部数据及预训练模型，不可以进行人工端点检测。 相关比赛 Kag">
<meta property="og:type" content="article">
<meta property="og:title" content="2021科大讯飞鸟鸣识别比赛总结">
<meta property="og:url" content="http://example.com/2023/12/16/birdsong/index.html">
<meta property="og:site_name" content="Yibo Liu">
<meta property="og:description" content="比赛介绍赛事链接：科大讯飞：鸟类鸣叫声识别挑战赛 任务描述：鸟类鸣叫声识别挑战赛旨在增强自动鸟类鸣叫声识别技术，预测出每个测试音频中出现的鸟类物种。测试音频文件只包含单一的鸟类物种，预测在音频文件级别进行，不需要开始和结束的时间戳，属于单标签分类任务。 数据说明：训练数据集包含100类鸟声数据，存在类别不均衡，真实背景噪音。不可以使用外部数据及预训练模型，不可以进行人工端点检测。 相关比赛 Kag">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img-blog.csdnimg.cn/46e4da89282143f79fe8690b01346100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1enVydUhhbnl1,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/9c7e9635e4644ad7a9df57791b28dba3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1enVydUhhbnl1,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/wav_img.png/">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/mfcc.png/">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/python_speech.png">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/logmelspec.png">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/mask.png">
<meta property="og:image" content="http://example.com/2023/12/16/birdsong/imgs/cvt_ptr.png/">
<meta property="article:published_time" content="2023-12-16T03:31:51.850Z">
<meta property="article:modified_time" content="2023-12-16T03:31:51.850Z">
<meta property="article:author" content="Yibo Liu">
<meta property="article:tag" content="Sound Recognition">
<meta property="article:tag" content="Audio Processing">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/46e4da89282143f79fe8690b01346100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1enVydUhhbnl1,size_16,color_FFFFFF,t_70">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>2021科大讯飞鸟鸣识别比赛总结</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post " href="/2023/12/16/hello-world/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Back to top " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2023/12/16/birdsong/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2023/12/16/birdsong/&text=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2023/12/16/birdsong/&is_video=false&description=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=2021科大讯飞鸟鸣识别比赛总结&body=Check out this article: http://example.com/2023/12/16/birdsong/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2023/12/16/birdsong/&name=2021科大讯飞鸟鸣识别比赛总结&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2023/12/16/birdsong/&t=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%B5%9B%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">比赛介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%AF%94%E8%B5%9B"><span class="toc-number">1.1.</span> <span class="toc-text">相关比赛</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">相关工作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">观察数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">2.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">3.</span> <span class="toc-text">特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MFCC"><span class="toc-number">3.1.</span> <span class="toc-text">MFCC</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">设计原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.2.</span> <span class="toc-text">基本流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text">尝试的几种实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogMelSpec"><span class="toc-number">3.2.</span> <span class="toc-text">LogMelSpec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%E6%96%B9%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">3.3.</span> <span class="toc-text">不同提取特征方式的对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vision-Transformer"><span class="toc-number">5.1.</span> <span class="toc-text">Vision Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%80%A7%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.1.1.</span> <span class="toc-text">验证性实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">预训练模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">5.2.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple-CNN"><span class="toc-number">5.2.1.</span> <span class="toc-text">Simple CNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E7%9A%84CNN%EF%BC%9AAuxSkipAttn"><span class="toc-number">5.2.2.</span> <span class="toc-text">改进的CNN：AuxSkipAttn</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">5.2.3.</span> <span class="toc-text">预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E6%95%B4%E5%8C%96%E9%9A%90%E7%A9%BA%E9%97%B4"><span class="toc-number">5.2.4.</span> <span class="toc-text">规整化隐空间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88LSTM-Transformer%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">CNN特征提取+序列模型（LSTM&#x2F;Transformer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90"><span class="toc-number">5.4.</span> <span class="toc-text">模型集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-number">6.</span> <span class="toc-text">致谢</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        2021科大讯飞鸟鸣识别比赛总结
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Yibo Liu</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-12-16T03:31:51.850Z" itemprop="datePublished">2023-12-16</time>
        
        (Updated: <time datetime="2023-12-16T03:31:51.850Z" itemprop="dateModified">2023-12-16</time>)
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Competition/">Competition</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Audio-Processing/" rel="tag">Audio Processing</a>, <a class="tag-link-link" href="/tags/Sound-Recognition/" rel="tag">Sound Recognition</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="比赛介绍"><a href="#比赛介绍" class="headerlink" title="比赛介绍"></a>比赛介绍</h2><p><strong>赛事链接：</strong><a target="_blank" rel="noopener" href="https://challenge.xfyun.cn/topic/info?type=bird-call">科大讯飞：鸟类鸣叫声识别挑战赛</a></p>
<p><strong>任务描述：</strong>鸟类鸣叫声识别挑战赛旨在增强自动鸟类鸣叫声识别技术，预测出每个测试音频中出现的鸟类物种。测试音频文件只包含单一的鸟类物种，预测在音频文件级别进行，不需要开始和结束的时间戳，属于单标签分类任务。</p>
<p><strong>数据说明：</strong>训练数据集包含100类鸟声数据，存在类别不均衡，真实背景噪音。不可以使用外部数据及预训练模型，不可以进行人工端点检测。</p>
<h3 id="相关比赛"><a href="#相关比赛" class="headerlink" title="相关比赛"></a>相关比赛</h3><ol>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/freesound-audio-tagging-2019">Kaggle: Freesound Audio Tagging 2019</a></p>
<p>通用音频分类，多标签。</p>
<p><em>The audio data is labeled using a vocabulary of 80 labels from Google’s AudioSet Ontology, covering diverse topics: Guitar and other Musical instruments, Percussion, Water, Digestive, Respiratory sounds, Human voice, Human locomotion, Hands, Human group actions, Insect, Domestic animals, Glass, Liquid, Motor vehicle (road), Mechanisms, Doors, and a variety of Domestic sounds.</em></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/birdsong-recognition">Kaggle: Cornell Birdcall Identification</a></p>
</li>
</ol>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ol>
<li><p><a target="_blank" rel="noopener" href="https://github.com/lRomul/argus-freesound">Kaggle: Freesound Audio Tagging 2019 第一名方案</a>：有完整的音频预处理步骤和模型训练，使用了作者自己写的轻量级深度学习框架<a target="_blank" rel="noopener" href="https://github.com/lRomul/argus">argus</a>。本方案使用了基于CNN的模型，它的数据增强的作用很大，值得参考。该方案是我们的最终提交版本的baseline。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data">FAT 2019 数据预处理流程</a>：上述第一名方案所参考的特征提取流程。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/ryanwongsa/kaggle-birdsong-recognition">Kaggle: Cornell Birdcall Identification 第一名方案</a>)：使用了事件检测的流程，用了语音事件检测的预训练模型PANN。该比赛的任务似乎与本比赛不是很相符，因此没有采用。可参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/GioDio/article/details/108673532?utm_medium=distribute.pc_relevant_download.none-task-blog-2~default~searchFromBaidu~default-10.test_version_3&depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-2~default~searchFromBaidu~default-10.test_version_">相关博客</a>。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b">Poland Birdsong Classification</a>：给出了一套数据处理流程，指出了数据现存的一些问题。</p>
</li>
</ol>
<h2 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h2><ol>
<li><p><strong>类别比例</strong>（数据包括train set和dev set）。坐标：类别-占比。</p>
<p><strong>分析：</strong> 需要处理类别不平衡，考虑（1）重采样（2） Focal Loss。  </p>
<h4 id=""><a href="#" class="headerlink" title=""></a></h4></li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/46e4da89282143f79fe8690b01346100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1enVydUhhbnl1,size_16,color_FFFFFF,t_70" alt="类别比例（包括train set和dev set）"></p>
<ol start="2">
<li><p><strong>音频长度</strong>。坐标：采样点数-样本数。</p>
<p><strong>分析：</strong> 需要考虑截取多长的时间片段作为输入。</p>
</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/9c7e9635e4644ad7a9df57791b28dba3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1enVydUhhbnl1,size_16,color_FFFFFF,t_70" alt="音频长度"></p>
<h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>语音的特征提取主要使用MFCC（梅尔倒谱系数），实际应用中发现使用log梅尔谱系数的情况也较多，同时通过比较两种方法得到的频谱图，本次比赛我们采用的是log梅尔谱系数。建议阅读参考资料 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/luoqingyu/p/5929389.html">博客园 - 数字信号处理–傅里叶变换</a> 和 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88625876">知乎 - 语音识别第4讲：语音特征参数MFCC</a>。</p>
<h3 id="MFCC"><a href="#MFCC" class="headerlink" title="MFCC"></a>MFCC</h3><h4 id="设计原理"><a href="#设计原理" class="headerlink" title="设计原理"></a>设计原理</h4><p><em>根据人耳听觉机理的研究发现，人耳对不同频率的声波有不同的听觉敏感度。从200Hz到5000Hz的语音信号对语音的清晰度影响对大。两个响度不等的声音作用于人耳时，则响度较高的频率成分的存在会影响到对响度较低的频率成分的感受，使其变得不易察觉，这种现象称为掩蔽效应。由于频率较低的声音在内耳蜗基底膜上行波传递的距离大于频率较高的声音，故一般来说，低音容易掩蔽高音，而高音掩蔽低音较困难。在低频处的声音掩蔽的临界带宽较高频要小。所以，人们从低频到高频这一段频带内按临界带宽的大小由密到疏安排一组带通滤波器，对输入信号进行滤波。将每个带通滤波器输出的信号能量作为信号的基本特征，对此特征经过进一步处理后就可以作为语音的输入特征。</em></p>
<h4 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h4><p>连续语音 -&gt; 预加重 -&gt;分帧 -&gt; 加窗 -&gt; FFT -&gt; Mel滤波器组 -&gt; 对数运算 -&gt; DCT（离散余弦变换）</p>
<p>（1）<strong>预加重</strong>：增强高频部分，即通过一个高通滤波器。频域变换为 $H(z)=1-\mu z^{-1}$，对应的时域变换为$y(t)=x(t)-\alpha x(t)$，这是实际计算时使用的。<br>（2）<strong>分帧</strong>：信号的频谱随时间变化，因此对整个信号进行傅立叶变换没有意义。假设频率在很短的时间内是平稳的，所以在短时间帧内进行傅里叶变换。<br>（3）<strong>加窗</strong>：以增加帧左端和右端的连续性。为了抵消FFT所假设的数据是无限的，并减少频谱泄漏。<br>（4）<strong>FFT</strong>：时域-&gt;频域<br>（5）<strong>Mel滤波器组</strong>：一系列滤波器，对不同频率设置不同的门限。<br>（6）<strong>对数运算</strong><br>（7）<strong>DCT（离散余弦变换）</strong>：去除一些变化过快的系数，这些系数在ASR任务中没有帮助。</p>
<h4 id="尝试的几种实现"><a href="#尝试的几种实现" class="headerlink" title="尝试的几种实现"></a>尝试的几种实现</h4><ol>
<li><code>librosa.feature.mfcc</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">signal, sample_rate = librosa.load(wav_file)</span><br><span class="line">mfcc_feat = librosa.feature.mfcc(signal,sr=sample_rate,n_mfcc=<span class="number">40</span>)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>python_speech_features.mfcc</code></li>
</ol>
<p><strong>Note:</strong> 以上两种实现不包括预加重。</p>
<ol start="3">
<li>手动实现</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.fftpack <span class="keyword">import</span> dct</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_mfcc_feature</span>(<span class="params">signal,sample_rate,n_mel_flt=<span class="number">40</span>,n_ceps=<span class="number">40</span></span>):</span></span><br><span class="line">    original_signal=signal  <span class="comment"># signal[0:int(2.5*sample_rate)]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># pre emphasise</span></span><br><span class="line">    pre_emphasis = <span class="number">0.97</span></span><br><span class="line">    emphasized_signal = np.append(original_signal[<span class="number">0</span>], original_signal[<span class="number">1</span>:] - pre_emphasis * original_signal[:-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># frame</span></span><br><span class="line">    frame_size = <span class="number">0.025</span></span><br><span class="line">    frame_stride = <span class="number">0.1</span></span><br><span class="line">    frame_length = <span class="built_in">int</span>(<span class="built_in">round</span>(frame_size*sample_rate))</span><br><span class="line">    frame_step = <span class="built_in">int</span>(<span class="built_in">round</span>(frame_stride*sample_rate)) </span><br><span class="line">    signal_length = <span class="built_in">len</span>(emphasized_signal)</span><br><span class="line">    num_frames = <span class="built_in">int</span>(np.ceil(<span class="built_in">float</span>(np.<span class="built_in">abs</span>(signal_length-frame_length))/frame_step))</span><br><span class="line">    pad_signal_length = num_frames * frame_step + frame_length</span><br><span class="line">    pad_signal = np.append(emphasized_signal, np.zeros((pad_signal_length - signal_length)))</span><br><span class="line">    indices = np.tile(np.arange(<span class="number">0</span>,frame_length),(num_frames,<span class="number">1</span>))+np.tile(np.arange(<span class="number">0</span>,num_frames*frame_step,frame_step), (frame_length, <span class="number">1</span>)).T</span><br><span class="line">    frames = pad_signal[np.mat(indices).astype(np.int32, copy=<span class="literal">False</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add window</span></span><br><span class="line">    frames *= np.hamming(frame_length)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FFT</span></span><br><span class="line">    NFFT = <span class="number">512</span></span><br><span class="line">    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  <span class="comment"># Magnitude of the FFT</span></span><br><span class="line">    pow_frames = (<span class="number">1.0</span> / NFFT) * (mag_frames ** <span class="number">2</span>)  <span class="comment"># 功率谱</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mel filter -&gt; fbanks</span></span><br><span class="line">    low_freq_mel = <span class="number">0</span></span><br><span class="line">    nfilt = n_mel_flt</span><br><span class="line">    high_freq_mel = (<span class="number">2595</span> * np.log10(<span class="number">1</span> + (sample_rate / <span class="number">2</span>) / <span class="number">700</span>))</span><br><span class="line">    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + <span class="number">2</span>)  <span class="comment"># Equally spaced in Mel scale</span></span><br><span class="line">    hz_points = (<span class="number">700</span> * (<span class="number">10</span>**(mel_points / <span class="number">2595</span>) - <span class="number">1</span>))  <span class="comment"># Convert Mel to Hz</span></span><br><span class="line">    <span class="built_in">bin</span> = np.floor((NFFT + <span class="number">1</span>) * hz_points / sample_rate)</span><br><span class="line">    fbank = np.zeros((nfilt, <span class="built_in">int</span>(np.floor(NFFT / <span class="number">2</span> + <span class="number">1</span>))))</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nfilt + <span class="number">1</span>):</span><br><span class="line">        f_m_minus = <span class="built_in">int</span>(<span class="built_in">bin</span>[m - <span class="number">1</span>])   <span class="comment"># left</span></span><br><span class="line">        f_m = <span class="built_in">int</span>(<span class="built_in">bin</span>[m])             <span class="comment"># center</span></span><br><span class="line">        f_m_plus = <span class="built_in">int</span>(<span class="built_in">bin</span>[m + <span class="number">1</span>])    <span class="comment"># right</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(f_m_minus, f_m):</span><br><span class="line">            fbank[m - <span class="number">1</span>, k] = (k - <span class="built_in">bin</span>[m - <span class="number">1</span>]) / (<span class="built_in">bin</span>[m] - <span class="built_in">bin</span>[m - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(f_m, f_m_plus):</span><br><span class="line">            fbank[m - <span class="number">1</span>, k] = (<span class="built_in">bin</span>[m + <span class="number">1</span>] - k) / (<span class="built_in">bin</span>[m + <span class="number">1</span>] - <span class="built_in">bin</span>[m])</span><br><span class="line">    filter_banks = np.dot(pow_frames, fbank.T)</span><br><span class="line">    filter_banks = np.where(filter_banks == <span class="number">0</span>, np.finfo(<span class="built_in">float</span>).eps, filter_banks)  <span class="comment"># Numerical Stability</span></span><br><span class="line">    filter_banks = <span class="number">20</span> * np.log10(filter_banks)  <span class="comment"># dB</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DCT -&gt; mfcc</span></span><br><span class="line">    num_ceps = n_ceps</span><br><span class="line">    mfcc = dct(filter_banks, <span class="built_in">type</span>=<span class="number">2</span>, axis=<span class="number">1</span>, norm=<span class="string">&#x27;ortho&#x27;</span>)[:, <span class="number">1</span> : (num_ceps + <span class="number">1</span>)]</span><br><span class="line">    (nframes, ncoeff) = mfcc.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># normalize mfcc</span></span><br><span class="line">    mfcc -= (np.mean(mfcc, axis=<span class="number">0</span>) + <span class="number">1e-8</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mfcc  <span class="comment"># (n_frames, n_features)</span></span><br></pre></td></tr></table></figure>

<h3 id="LogMelSpec"><a href="#LogMelSpec" class="headerlink" title="LogMelSpec"></a>LogMelSpec</h3><p>LogMelSpec与MFCC区别在于它没有DCT。可用的实现是<code>librosa.features.melspectrogram</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">signal, sample_rate = librosa.load(wav_file)</span><br><span class="line">melspec = librosa.feature.melspectrogram(signal,sr=sample_rate,n_fft=<span class="number">1024</span>,hop_length=<span class="number">512</span>,n_mels=<span class="number">128</span>)</span><br><span class="line">logmelspec = librosa.power_to_db(melspec)</span><br></pre></td></tr></table></figure>

<h3 id="不同提取特征方式的对比"><a href="#不同提取特征方式的对比" class="headerlink" title="不同提取特征方式的对比"></a>不同提取特征方式的对比</h3><p>我们希望通过观察特征图像来判断那种特征更合适。参与对比的有mfcc的两种实现和logmelspec，他们的特征数量（即梅尔滤波器数量）均设置为128，实验代码详见<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/preprocessing/feature_extract.ipynb">notebook</a>。通过对比，我们最终选用logmelspec作为特征。</p>
<p><strong>wav时域图：</strong></p>
<img src="imgs/wav_img.png/" alt="img" style="zoom:50%;" />

<p><strong>librosa.features.mfcc:</strong></p>
<img src="imgs/mfcc.png/" style="zoom:50%;" />

<p><strong>python_speech_features:</strong></p>
<img src="imgs/python_speech.png" style="zoom:50%;" />

<p><strong>librosa.features.logmelspec:</strong></p>
<img src="imgs/logmelspec.png"  />

<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>目前对语音信号的建模方式为：对整段音频信号提取频谱图，将频谱图视为图像，沿该图像的时间轴取一小段定长片段，放入处理图像的模型中，如CNN等。因此数据增强的对象是提取的特征图像，数据增强会包括截取、加噪等。特征的形状：dim_x=整个音频的帧数，dim_y=特征数量(滤波器个数)。</p>
<p>我们主要参考<a target="_blank" rel="noopener" href="https://github.com/lRomul/argus-freesound#augmentations">Kaggle: Freesound Audio Tagging 2019 第一名方案</a>，所使用的数据增强包括以下步骤：</p>
<ol>
<li><p><strong>截取片段</strong></p>
<p>由于鸟鸣在一段音频中的出现是周期性的，而数据集中音频的长度不一，因此对每段音频<em><strong>随机</strong></em>截取一小段（256帧）作为训练数据。长度不够256帧的要补齐。这一步有许多之的改进的地方：</p>
<p>（1）具体帧数可以更改，256帧是照搬的freesound方案，应该统计我们数据中每一声鸟鸣的长度是多少，截取的长度要综合考虑鸟鸣长度和鸟鸣间隔。</p>
<p>（2）不一定要随机截取，可以辅以端点检测和静音检测（题目中说不可以使用人工端点检测，但我们可以使用自动的），以避免取到不包含鸟鸣的片段。</p>
<p>（3）进而引发的思考是，截取片段的方法只用到了局部信息，没有用到全局信息（如鸟鸣间隔，鸟一共名叫多少声等等），因此后续我们尝试了序列模型（CNN特征提取器+LSTM后端）来利用全局信息。</p>
</li>
</ol>
<ol start="2">
<li><p><strong>随机缩放</strong></p>
<p>freesound方案作者称该步骤提升显著，我们照搬了。</p>
</li>
<li><p><strong>加噪</strong></p>
<p>我认为有两种思路，一种是给图像加噪，即在特征图像上加矩形/sin函数形的mask；另一种是对原始音频添加白噪声/粉噪声等。</p>
<p>我们截至比赛截至时只尝试了freesound使用的矩形mask和我们增加的sin函数形mask。（但提交版本暂未加入sin函数形mask，也尚未实验验证其效果。）图像加噪的效果如下：</p>
</li>
</ol>
   <img src="imgs/mask.png"  />
   
   原始音频加噪的方法可以参考[Cornell Birdcall Identification 第一名方案的数据增强](https://github.com/ryanwongsa/kaggle-birdsong-recognition#data-augmentation)， 我个人认为这种加噪方法更符合直觉，值得尝试。

<p><strong>其他尝试：预加重</strong></p>
<p>预加重是在提取特征之前对原始音频的处理，增强高频部分。librosa的logmelspec特征并没有预加重的步骤，因此我们手动添加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">signal,sample_rate = librosa.load(fname)</span><br><span class="line">pre_emphasis = <span class="number">0.97</span></span><br><span class="line">pre_emphasised_signal = np.append(signal[<span class="number">0</span>], signal[<span class="number">1</span>:] - pre_emphasis * signal[:-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>增加预加重后valid acc提升了1.89%（0.71-&gt;0.7389）。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ul>
<li>Vision Transformer</li>
<li>CNN </li>
<li>CNN特征提取+序列模型（LSTM/Transformer）</li>
</ul>
<h3 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h3><p>考虑使用Vision Transformer的动机是，Kaggle Freesound和Cornell Birdcall都是2019或2020年的比赛，而Transformer是2019年底才流行起来、Vision Transformer更是2020年才出现，所以以前这些比赛获奖者所采用的CNN方案未必是当下最好的方案。因此我们决定直接上强有力的模型，Vision Transformer。对与分类任务，有<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">ViT</a>和CvT。</p>
<p><strong>Vision Transformer原理讲解：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348593638">知乎 - Vision Transformer , Vision MLP超详细解读 (原理分析+代码解读)</a></p>
<p><strong>可用的开源实现：</strong></p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://github.com/rwightman/pytorch-image-models">timm</a>：一个包含各种视觉模型的库。使用方法可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350837279">知乎 - 视觉Transformer优秀开源工作：timm库vision transformer代码解读</a>。官方介绍：<em>PyTorch image models, scripts, pretrained weights – ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more</em>。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch">vit-pytorch</a>：一个包含各种Vision Transformer的库。</p>
</li>
</ol>
<h4 id="验证性实验"><a href="#验证性实验" class="headerlink" title="验证性实验"></a>验证性实验</h4><p>为了测试ViT的可用性，先在<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data">Kaggle猫狗分类数据集</a>上跑了ViT，准确率为~0.6，这并不高。在我们的Bird 4k数据集上（为方便调试取了一个小数据集，大小为4k，由每类别随机取等数量的样本得到）上得到的准确率为~0.02。</p>
<p>为了测试CvT的可用性，先在通用图像分类数据集<a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">caltech256</a>上跑了CvT，准确率为~0.15。</p>
<h4 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h4><p>Transformer是在使用了预训练后才大放异彩（BERT），因此这里我们也考虑预训练。因为没有找到现成的Vision Transformer在图像或者在语音数据上的预训练模型，我们打算自己写预训练。能想到的预训练的方法有两种：</p>
<ol>
<li><p><strong>自编码器</strong>。这是非常符合直觉的，但可能不好训练，因为要恢复的内容太多，CvT可能会很复杂。</p>
</li>
<li><p><strong>预测Mask的任务</strong>（像BERT那样）。训练起来可能会更容易（因为预测的内容只是局部），但关键是设计的预测任务要确保合理有效、能帮到后面的分类任务。</p>
</li>
</ol>
<p>首先我们为CvT设计了自编码器：</p>
<img src="imgs/cvt_ptr.png/" alt="cvt pretrain" style="zoom: 50%;" />

<p>（上面是预训练模型，下面是分类模型）</p>
<p>自编码器的解码器目前使用了一个直觉设计的CNN，这里有修改的空间。</p>
<p><strong>实验结果：</strong></p>
<p>对于caltech256，预训练使用的数据是其训练集，使用了预训练后分类准确率没有提高。可能的原因是预训练的数据不够多，Transformer这种复杂模型很容易在小数据集上过拟合。</p>
<p>对于Bird 4k数据集，预训练使用的数据也是4k数据集，无预训练分类准确率~0.02，有预训练分类准确率~0.076，这说明预训练是有效的。训练过程中训练集准确率达0.9以上，说明过拟合了。</p>
<p><strong>关于Vision Transformer预训练模型的结论：</strong></p>
<p>预训练并非是无效的。实验中观察到预训练模型loss的下降非常缓慢，这可能是由于需要大量数据和时间，而我们数据不够大、也仅跑了最多几百个epoch来观察。BERT的训练花费了3天3夜，起初的效果也并不显著。</p>
<p>如不使用预训练则效果比不过CNN，如想要预训练有效则需要大量数据和时间。该预训练模型的改进方向是：（1）搜集更多外部数据（鸟鸣的，或至少是用于音频分类的）（2）修改自编码器的Decoder结构。于是我们暂别Transformer，转投CNN的怀抱。</p>
<h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>从这里开始，我们使用完整数据集训练。在训练Vision Transformer时，为了节约时间，我们使用的是一个大小为4k子数据集。</p>
<h4 id="Simple-CNN"><a href="#Simple-CNN" class="headerlink" title="Simple CNN"></a>Simple CNN</h4><p>首先尝试了一个简单的CNN并使用了batchnorm（<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/model_SimpleCNN_dataAug_Bird.ipynb">notebook</a>），在完整数据集上未使用数据增强得到的结果是valid acc=0.58，使用数据增强得到的结果是valid acc=0.68。此外，也尝试了ResNet101和ResNet50，均未达到更好效果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Classifier(nn.Module):</span><br><span class="line">    def __init__(self, num_classes):</span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv &#x3D; nn.Sequential(</span><br><span class="line">            ConvBlock(in_channels&#x3D;3, out_channels&#x3D;64),    # (B,64,64,128)</span><br><span class="line">            ConvBlock(in_channels&#x3D;64, out_channels&#x3D;128),  # (B,128,32,64)</span><br><span class="line">            ConvBlock(in_channels&#x3D;128, out_channels&#x3D;256), # (B,256,16,32)</span><br><span class="line">            ConvBlock(in_channels&#x3D;256, out_channels&#x3D;512), # (B,512,8,16)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.linear1 &#x3D; nn.Linear(32,1)</span><br><span class="line">        </span><br><span class="line">        self.fc &#x3D; nn.Sequential(</span><br><span class="line">            nn.Dropout(0.4),</span><br><span class="line">            nn.Linear(512, 128),</span><br><span class="line">            nn.PReLU(),</span><br><span class="line">            nn.BatchNorm1d(128),</span><br><span class="line">            nn.Dropout(0.1),</span><br><span class="line">            nn.Linear(128, num_classes),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x &#x3D; self.conv(x)       #[B,512,8,16]</span><br><span class="line">        x &#x3D; F.avg_pool2d(x, 2) #[B,512,4,8]</span><br><span class="line">        #x,_ &#x3D; x.max(dim&#x3D;-1)    #[B,512,4,1]</span><br><span class="line">        x &#x3D; rearrange(x,&#39;B C H W -&gt; B C (H W)&#39;) #[B,512,32]</span><br><span class="line">        x &#x3D; self.linear1(x)    #[B,512,1]</span><br><span class="line">        x &#x3D; x.squeeze(-1)      #[B,512]</span><br><span class="line">        x &#x3D; self.fc(x)         #[B,num_class]</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h4 id="改进的CNN：AuxSkipAttn"><a href="#改进的CNN：AuxSkipAttn" class="headerlink" title="改进的CNN：AuxSkipAttn"></a>改进的CNN：AuxSkipAttn</h4><p>进而尝试了freesound方案的模型AuxSkipAttn（<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_dataAug_Bird.ipynb">notebook</a>）作为baseline，简言之它是一个添加了Attention，Skip connection和Auxiliary classifier的CNN，得到的结果是valid acc=0.72，提交测试结果test acc=0.64。这个结果说明测试数据和训练数据分布不一致，因此训练数据不平衡的问题需要引起重视并解决。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from models.aux_skip_attention import AuxSkipAttention</span><br><span class="line"></span><br><span class="line">class Classifier(nn.Module):</span><br><span class="line">    def __init__(self, num_classes):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.aux_skip_attn &#x3D; AuxSkipAttention(num_classes&#x3D;num_classes,base_size&#x3D;64,dropout&#x3D;0.4,ratio&#x3D;16,kernel_size&#x3D;7,last_filters&#x3D;8,last_fc&#x3D;4)</span><br><span class="line">        self.aux_weights &#x3D; [1.0, 0.4, 0.2, 0.1]</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x, aux3, aux2, aux1 &#x3D; self.aux_skip_attn(x)</span><br><span class="line">        return x, aux3, aux2, aux1</span><br></pre></td></tr></table></figure>

<h4 id="预训练模型-1"><a href="#预训练模型-1" class="headerlink" title="预训练模型"></a>预训练模型</h4><p>图像领域CNN的预训练模型已经广泛使用，因次希望音频特征图也能用上。首先，挪用通用图像分类的预训练模型如ResNet等是不可取的，因为图像的差异太大。其次，我们尝试寻找通用的音频特征图的CNN预训练模型，未果。因此我们决定自己训一个鸟鸣音频的预训练模型。</p>
<p>为了扩大训练数据，我们搜集了许多外部数据集，包括：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/freesound-audio-tagging-2019/data">Kaggle Freesound Auto Tagging 2019 dataset</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/rtatman/british-birdsong-dataset">British Birdsong Dataset</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/birdsong-recognition/data">Kaggle: Cornell Birdcall Identification dataset</a></p>
</li>
</ul>
<p>这次我们采用了将预测Mask作为任务的预训练方案(<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/model_MaskPretrain_dataAug_Bird.ipynb">notebook</a>)。mask的长度取为32。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class ConvDecoder(nn.Module):</span><br><span class="line">    def __init__(self,input_dim&#x3D;1024,outsz_h&#x3D;128,outsz_w&#x3D;10):</span><br><span class="line">        super(ConvDecoder,self).__init__()</span><br><span class="line">        self.conv1 &#x3D; torch.nn.Conv1d(in_channels&#x3D;1,out_channels&#x3D;input_dim,kernel_size&#x3D;1,stride&#x3D;1,padding&#x3D;0)</span><br><span class="line">        self.conv2 &#x3D; torch.nn.Conv2d(in_channels&#x3D;1,out_channels&#x3D;3,kernel_size&#x3D;5,stride&#x3D;1,padding&#x3D;2)</span><br><span class="line">        self.conv3 &#x3D; torch.nn.Conv2d(in_channels&#x3D;3,out_channels&#x3D;3,kernel_size&#x3D;5,stride&#x3D;1,padding&#x3D;2)</span><br><span class="line">        self.conv4 &#x3D; torch.nn.Conv2d(in_channels&#x3D;3,out_channels&#x3D;3,kernel_size&#x3D;5,stride&#x3D;1,padding&#x3D;2)</span><br><span class="line">        self.conv5 &#x3D; torch.nn.Conv2d(in_channels&#x3D;3,out_channels&#x3D;3,kernel_size&#x3D;5,stride&#x3D;1,padding&#x3D;2)</span><br><span class="line">        self.conv6 &#x3D; torch.nn.Conv2d(in_channels&#x3D;3,out_channels&#x3D;12,kernel_size&#x3D;5,stride&#x3D;1,padding&#x3D;2)</span><br><span class="line">        self.dropout &#x3D; None</span><br><span class="line">        self.batchnorm &#x3D; None</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x &#x3D; F.relu(self.conv1(x))</span><br><span class="line">        x &#x3D; x.unsqueeze(dim&#x3D;1)  # [4, 1, 1024, 1024]</span><br><span class="line">        x &#x3D; F.relu(F.max_pool2d(self.conv2(x),kernel_size&#x3D;2))  # [4, 3, 512, 512]</span><br><span class="line">        x &#x3D; F.relu(F.max_pool2d(self.conv3(x),kernel_size&#x3D;2))  # [4, 3, 256, 256]</span><br><span class="line">        x &#x3D; F.relu(F.max_pool2d(self.conv4(x),kernel_size&#x3D;2)) # [4, 3, 128, 128]</span><br><span class="line">        x &#x3D; F.relu(F.max_pool2d(self.conv5(x),kernel_size&#x3D;2))  # [4, 3, 64, 64]</span><br><span class="line">        x &#x3D; F.relu(F.max_pool2d(self.conv6(x),kernel_size&#x3D;2))   #[4, 3*4, 32, 32]     </span><br><span class="line">        x &#x3D; rearrange(x,&#39;B (c1 c2) H W -&gt; B c2 (c1 H) W&#39;,c1&#x3D;4) #[4,4*128,32]  -&gt;  [4,3,128,32]</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from models.aux_skip_attention import AuxSkipAttention</span><br><span class="line"></span><br><span class="line">class PreTrainer(nn.Module):</span><br><span class="line">    def __init__(self,hid_dim&#x3D;1024,mask_h&#x3D;128,mask_w&#x3D;10):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.aux_skip_attn &#x3D; AuxSkipAttention(num_classes&#x3D;hid_dim,base_size&#x3D;64,dropout&#x3D;0.4,ratio&#x3D;16,kernel_size&#x3D;7,last_filters&#x3D;8,last_fc&#x3D;4)</span><br><span class="line">        self.aux_weights &#x3D; [1.0, 0.4, 0.2, 0.1]</span><br><span class="line">        self.decoder &#x3D; ConvDecoder(input_dim&#x3D;hid_dim,outsz_h&#x3D;mask_h,outsz_w&#x3D;mask_w)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        hid, aux3, aux2, aux1 &#x3D; self.aux_skip_attn(x)  # hid&#x3D;(batch_size,1024)</span><br><span class="line">        hid &#x3D; hid.unsqueeze(dim&#x3D;1)</span><br><span class="line">        out &#x3D; self.decoder(hid)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>

<p><strong>实验结果：</strong>预训练loss下降非常缓慢，训练30epoch后停止，分类器valid acc=0.7。可能的原因：预训练模型没有训好。可改进的点：（1）ConvDecoder可以更好地设计（这里只是凭直觉设计的）（2）预训练使用的4个数据集的数据量为42011，原始数据量为10906，可能仍不够多。</p>
<h4 id="规整化隐空间"><a href="#规整化隐空间" class="headerlink" title="规整化隐空间"></a>规整化隐空间</h4><p>希望各类别的数据能符合高斯混合分布，因此需要对隐空间做一个变换来使之符合这种分布。我们分别尝试了：</p>
<ul>
<li><p><strong>Deep Normalize Flow (DNF)</strong> (<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_DNF_dataAug_Bird.ipynb">notebook</a>)</p>
<p>Flow模型是一种生成式模型，这里利用了其归一化的原理。</p>
<p><em>A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow,[1] which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one. – <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Flow-based_generative_model">Wikipedia</a></em></p>
</li>
<li><p><strong>Wasserstein Auto-Encoder (WAE)</strong> (<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_WLoss_dataAug_Bird.ipynb">notebook</a>) </p>
</li>
</ul>
<h3 id="CNN特征提取-序列模型（LSTM-Transformer）"><a href="#CNN特征提取-序列模型（LSTM-Transformer）" class="headerlink" title="CNN特征提取+序列模型（LSTM/Transformer）"></a>CNN特征提取+序列模型（LSTM/Transformer）</h3><p>思路是将将整段特征图像切分为若干帧长为128的片段（此前是整段特征图像随机取一个片段，此处要输入全部片段），将AuxAttnSkip作为编码器为每一个128帧长度的片段编码，将编码后的特征序列输入LSTM或Transformer，就像输入embedding后的文本序列。这样做的目的是利用全局信息，而不只是局部信息。该方法有待尝试。</p>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>根据许多人的比赛经验，bagging/boosting/stacking能显著提高效果。这里我们仅仅是将同一个模型以不同的随机种子在测试集上预测5或10次，并投票取结果，就获得了test acc从0.64到0.72的提升（该模型的valid acc=0.72）。详见<a target="_blank" rel="noopener" href="https://github.com/zll17/BirdRec/blob/main/train_script/pred_AuxSkipAttn_Bird.ipynb">notebook</a>。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢队友<a target="_blank" rel="noopener" href="https://github.com/zll17">@方阿</a>的鼎力支持，他总是有源源不断的好点子和超强的行动力。与他的合作是对我理想中的合作模式的卓越实践。期待下一次！ :-)</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%B5%9B%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">比赛介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%AF%94%E8%B5%9B"><span class="toc-number">1.1.</span> <span class="toc-text">相关比赛</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">相关工作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">观察数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">2.0.1.</span> <span class="toc-text"></span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">3.</span> <span class="toc-text">特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MFCC"><span class="toc-number">3.1.</span> <span class="toc-text">MFCC</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">设计原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.2.</span> <span class="toc-text">基本流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text">尝试的几种实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogMelSpec"><span class="toc-number">3.2.</span> <span class="toc-text">LogMelSpec</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%E6%96%B9%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">3.3.</span> <span class="toc-text">不同提取特征方式的对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vision-Transformer"><span class="toc-number">5.1.</span> <span class="toc-text">Vision Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%80%A7%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.1.1.</span> <span class="toc-text">验证性实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.1.2.</span> <span class="toc-text">预训练模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">5.2.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple-CNN"><span class="toc-number">5.2.1.</span> <span class="toc-text">Simple CNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E7%9A%84CNN%EF%BC%9AAuxSkipAttn"><span class="toc-number">5.2.2.</span> <span class="toc-text">改进的CNN：AuxSkipAttn</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">5.2.3.</span> <span class="toc-text">预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E6%95%B4%E5%8C%96%E9%9A%90%E7%A9%BA%E9%97%B4"><span class="toc-number">5.2.4.</span> <span class="toc-text">规整化隐空间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88LSTM-Transformer%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">CNN特征提取+序列模型（LSTM&#x2F;Transformer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90"><span class="toc-number">5.4.</span> <span class="toc-text">模型集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-number">6.</span> <span class="toc-text">致谢</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2023/12/16/birdsong/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2023/12/16/birdsong/&text=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2023/12/16/birdsong/&is_video=false&description=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=2021科大讯飞鸟鸣识别比赛总结&body=Check out this article: http://example.com/2023/12/16/birdsong/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2023/12/16/birdsong/&title=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2023/12/16/birdsong/&name=2021科大讯飞鸟鸣识别比赛总结&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2023/12/16/birdsong/&t=2021科大讯飞鸟鸣识别比赛总结"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2023
    Yibo Liu
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/projects_url">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
